{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42da89-9dbc-4f33-be72-2a617719b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "train_txt_path = \"../2_str_processor/nl_list_aozora.txt\"\n",
    "val_txt_path = train_txt_path\n",
    "ds_train = datasets.load_dataset('text', data_files=[train_txt_path],split=\"train[:90%]\")\n",
    "ds_valid = datasets.load_dataset('text', data_files=[val_txt_path],split=\"train[90%:]\")\n",
    "\n",
    "text_datasets = datasets.DatasetDict({\n",
    "    \"train\": ds_train,\n",
    "    \"valid\": ds_valid,\n",
    "})\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" #tokenizerの警告を消す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462248c3-65ef-4361-8fe2-a5b3ebed40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_datasets['train']['text'][0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5410cc1-6bae-497a-8377-3d57681194a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "spm_folder = \"../3_make_tokenizer/sentencepiece/\"\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file = spm_folder + \"spm_tokenizer.json\",\n",
    ")\n",
    "print(tokenizer.encode(\"[NL]\"),tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1206e3-0540-4a64-b026-0221b2b0c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    #今回は1024トークンをフルで使うようにデータを詰める\n",
    "    all_ids = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        all_ids+=input_ids\n",
    "\n",
    "    buf_ids = []\n",
    "    batch_list = []\n",
    "    for tkn in all_ids:\n",
    "        buf_ids.append(tkn)\n",
    "        if len(buf_ids)==1024:\n",
    "            batch_list.append(buf_ids)\n",
    "            buf_ids = []\n",
    "    return {\"input_ids\": batch_list}\n",
    "\n",
    "tokenized_datasets = text_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=text_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets.save_to_disk('./tokenized_datasets')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
